# -*- coding: utf-8 -*-
"""Drishti_Assignment2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yLTGqHRl7cBr6zR-0HiLohZpFeiYY7TV
"""

#Drishti
#2020EEB1168
#Assignment-2

import numpy as np
import skimage
import pandas as pd
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
from skimage.io import imread, imshow
from skimage.color import rgb2gray
import cv2
import random

data = keras.datasets.fashion_mnist
(xtrain, ytrain), (xtest, ytest) = data.load_data()

plt.imshow(xtrain[3])
plt.show()

ytrain[3]

sift=cv2.xfeatures2d.SIFT_create()
keypoints_array=[]
descriptors_array=[]

#Feature extraction using SIFT Detector

def feature_extraction_using_SIFT(image):
  temp_arr=[]
  keypoint,descriptors=sift.detectAndCompute(img,None)
  keypoints_array.append(keypoint)
  descriptors_array.append(descriptors)
  return 0

xtemp=xtrain
ytemp=ytrain

# feature_extr=feature_extraction_using_SIFT(xtrain)
for img in xtemp:
  if img is None:
    # print('lol')
    break
  else:
    x=feature_extraction_using_SIFT(img)
      #  print('lol')
      # descriptors_array.extend(temp_arr2)

total_descriptors=[]
for desc in descriptors_array:
  if desc is None:
    continue
  else:
    for one_desc in desc:
     total_descriptors.append(one_desc)

total_descriptors=np.stack(total_descriptors)

total_descriptors.shape

# function to find the euclidian distance between two points
def distance(point1,point2):
  dist=np.sqrt(np.sum((point1 - point2)**2, axis=1))
  return dist

# To find the frequency of maximum occuring elemnt
def max_frequency(arr):
  return max(set(arr), key=arr.count)

def fit_data(number_of_clusters,number_of_examples,number_of_features,arr):
  centroids_to_take=random_centroids(number_of_clusters,number_of_features,number_of_examples,arr)
  max_iterations=100
  for iterations in range(0,max_iterations):
    clusters=make_clusters(number_of_clusters,centroids_to_take,arr)
    temp_centroids=centroids_to_take
    centroids_to_take=new_centroid(number_of_clusters,number_of_features,clusters,arr)
    visual_words=centroids_to_take
    difference=temp_centroids-centroids_to_take
    if not difference.any():
      break
  y_predicted=final_clusters(number_of_examples,arr,clusters)
  return y_predicted,visual_words

def new_centroid(number_of_clusters,number_of_features,clusters,arr):
  new_centroids = np.zeros((number_of_clusters, number_of_features))

  for index,cluster in enumerate(clusters):
    new_centre=np.mean(arr[cluster],axis=0)
    new_centroids[index]=new_centre

  return new_centroids

def final_clusters(number_of_examples,arr,clusters):
  # number_of_examples=len(arr)
  y=[]
  for i in range(0,number_of_examples):
    y.append(0)
  for cluster_number,cluster in enumerate(clusters):
    for samples in cluster:
      y[int(samples)]=cluster_number
  return y

def make_clusters(number_of_clusters,centroids,arr):
  clusters=[[]for _ in range(number_of_clusters)]
  i=0
  for img in arr:
    closest_centroid=np.argmin(np.sqrt(np.sum((img-centroids)**2, axis=1)))
    clusters[closest_centroid].append(i)
    i=i+1
  return clusters

def random_centroids(number_of_clusters,number_of_features,number_of_examples,arr):
  centroids=np.zeros((number_of_clusters,number_of_features))

  for k in range(0,number_of_clusters):
    centroid=arr[np.random.choice(range(number_of_examples))]
    centroids[k]=centroid
    
  return centroids

def KmeansClustering(number_of_examples,number_of_features,number_of_clusters,descriptor_arr):
  y_predicted,words=fit_data(number_of_clusters,number_of_examples,number_of_features,descriptor_arr)
  return y_predicted,words

def CreateVisualDictionary(k,descriptor_arr):
  number_of_examples,number_of_features=descriptor_arr.shape
  y_predicted,words=KmeansClustering(number_of_examples,number_of_features,k,descriptor_arr)
  return y_predicted,words

k=15

# elbow method for finding the optimal value of k
def calculate_error(data,maxk):
  error=[]
  for k in range(1,maxk+1):
    y_pred,centroids=CreateVisualDictionary(k,data)
    sse=0

    for i in range(len(data)):
      temp_center=y_pred[int(i)]
      sse+=np.linalg.norm(temp_center-y_pred[i])

    error.append(sse)
  return error

error=calculate_error(total_descriptors,k)
x=np.arange(1,k+1)
plt.title("Finding optimal value of k")
plt.xlabel("Range of k")
plt.ylabel("error")
plt.plot(x, error, color ="red")
plt.show()

y_predicted,visual_words=CreateVisualDictionary(k,total_descriptors)

from tempfile import TemporaryFile
dictionary = TemporaryFile()
np.save(dictionary, visual_words)

from google.colab import drive
drive.mount('/content/drive')

from sklearn import cluster
from sklearn import preprocessing
import plotly.express as px
fig = px.scatter(total_descriptors[:, 0], total_descriptors[:, 1], color=y_predicted)
fig.show()

from scipy.cluster.vq import vq

def ComputeHistogram(descriptors_array,visual_words,x,y):
  lol=[]
  x_new=[]
  y_new=[]
  for i in range(len(descriptors_array)):
    if descriptors_array[i] is None:
      continue
    else:
      x_new.append(x[i])
      y_new.append(y[i])
      img_visual_words, distance = vq(descriptors_array[i], visual_words)
      # type(img_descriptors)
      lol.append(img_visual_words)
    
   

  freq_vect=[]
  for img in lol:
    frequency_vect=np.zeros(k)
    for each_img in img:
      frequency_vect[each_img]+=1
    freq_vect.append(frequency_vect)
  freq_vect=np.stack(freq_vect)
      
  N=len(xtemp)
  df=np.sum(freq_vect>0,axis=0)
  idf=np.log(N/df)
  final_tfidf=freq_vect*idf
  x_new=np.array(x_new)
  y_new=np.array(y_new)
  return final_tfidf,x_new,y_new

def ComputeHistogramtest(descriptors_array,visual_words,x,y):
  lol=[]
  x_new=[]
  y_new=[]
  for i in range(len(descriptors_array)):
    if descriptors_array[i] is None:
      continue
    else:
      x_new.append(x[i])
      y_new.append(y[i])
      img_visual_words, distance = vq(descriptors_array[i], visual_words)
      # type(img_descriptors)
      lol.append(img_visual_words)
    
   

  freq_vect=[]
  for img in lol:
    frequency_vect=np.zeros(k)
    for each_img in img:
      frequency_vect[each_img]+=1
    freq_vect.append(frequency_vect)
  freq_vect=np.stack(freq_vect)
      
  return freq_vect,x_new,y_new

final_tfidf,x_new,y_new=ComputeHistogram(descriptors_array,visual_words,xtemp,ytemp)
plt.bar(list(range(k)), final_tfidf[5])
plt.show()

x_test_temp=xtest
y_temp_test=ytest

keypoints_array_test=[]
descriptors_array_test=[]

for img in x_test_temp:
  keypoint,descriptors=sift.detectAndCompute(img,None)
  keypoints_array_test.append(keypoint)
  descriptors_array_test.append(descriptors)

tf_idf_test,x_new_test,y_new_test=ComputeHistogramtest(descriptors_array_test,visual_words,x_test_temp,y_temp_test)

from numpy import dot
from numpy.linalg import norm

def cosine_similarity(arr1, arr2):
  cos_sim = np.dot(arr1,arr2.T)/(norm(arr1)*norm(arr2,axis=1))
  return cos_sim

tf_idf_test.shape

def MatchHistogram(tf_idf_test,final_tfidf):
  labels=[]
  index=[]
  for i in range (len(tf_idf_test)):
    temp=[]
    img=tf_idf_test[i]
    img2=final_tfidf
    cos_sim_i=cosine_similarity(img,final_tfidf)
    indx=np.argsort(-cos_sim_i)[:1]
    index.append(indx)
    labels.append(ytemp[indx[0]])
  return labels,index

labels_pred,index=MatchHistogram(tf_idf_test,final_tfidf)

from sklearn.metrics import accuracy_score
accuracy_score(y_new_test,labels_pred)

# finding accuracies of individual cataegories
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
accuracy_score_matrix_for_each_label=[]
precision_score_matrix_for_each_label=[]
recall_score_matrix_for_each_label=[]
for i in range(0,10):
  actual_val=[]
  found_val=[]
  for j in range(len(y_new_test)):
    if(y_new_test[j]==i):
      actual_val.append(y_new_test[j])
      found_val.append(labels_pred[j])
  accuracy=accuracy_score(actual_val,found_val)
  precision = precision_score(actual_val, found_val, average='micro') 
  recall = recall_score(actual_val, found_val, average='micro') 
  accuracy_score_matrix_for_each_label.append(accuracy)
  precision_score_matrix_for_each_label.append(precision)
  recall_score_matrix_for_each_label.append(recall)

precision_score_matrix_for_each_label

recall_score_matrix_for_each_label

accuracy_score_matrix_for_each_label

accuracy_score(y_new_test,labels_pred)

np.savetxt('/content/drive/MyDrive/Dictionary/visual_words.txt',visual_words)

